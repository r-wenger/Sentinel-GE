{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osgeo.gdal as gdal\n",
    "import osgeo.ogr as ogr\n",
    "import osgeo.osr as osr\n",
    "from osgeo.gdalconst import *\n",
    "\n",
    "# Numpy Import\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from skimage import morphology\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Others imports\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.ndimage import zoom\n",
    "import json\n",
    "import tifffile as tiff\n",
    "import shutil\n",
    "import zipfile\n",
    "from pyproj import Proj, transform\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import shutil\n",
    "import cv2\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentinel_tiles(path_shapefile_sentinel, field):\n",
    "    \"\"\"\n",
    "    Returning an array containing all tilenames in the shapefile tiling.\n",
    "    @params:\n",
    "        path_shapefile_sentinel   \t- Required  : path to the shapefile containing tiling\n",
    "        field                      \t- Required  : column name in the shapefile to extract tile number\n",
    "    \"\"\"\n",
    "    driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "    data_source = driver.Open(path_shapefile_sentinel, 0)\n",
    "    layer = data_source.GetLayer()\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for feature in layer:\n",
    "        _val = feature.GetField(field)\n",
    "        res.append(_val)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(path_to_zip_file, directory_to_extract_to):\n",
    "    \"\"\"\n",
    "    Unzip a file to a precise directory and remove it after process.\n",
    "    @params:\n",
    "        path_to_zip_file   \t        - Required  : Path to zip file\n",
    "        directory_to_extract_to     - Required  : Path to the directory where zip file should be extracted\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory_to_extract_to):\n",
    "        os.mkdir(directory_to_extract_to)\n",
    "\n",
    "    command_unzip = 'unzip -o ' + str(path_to_zip_file) + ' -d ' + str(directory_to_extract_to)\n",
    "    os.system(command_unzip)\n",
    "    command_del = 'rm ' + str(path_to_zip_file)\n",
    "    os.system(command_del)\n",
    "\n",
    "    print(datetime.now().isoformat() + 'Successfully extracted ' + str(path_to_zip_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    \"\"\"\n",
    "    Merging two dictionnaries\n",
    "    @params:\n",
    "        x   \t\t  - Required  : first dictionnary\n",
    "        z   \t\t  - Required  : second dictionnary\n",
    "    \"\"\"\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dic(tab_classes):\n",
    "    \"\"\"\n",
    "    Transform an array containing classes to a dictionnary\n",
    "    @params:\n",
    "        tab_classes   \t\t  - Required  : array containing classes definition\n",
    "    \"\"\"\n",
    "    dic_res = {}\n",
    "\n",
    "    for e in tab_classes:\n",
    "        _tmp = e.split(':')\n",
    "        dic_res[int(_tmp[0])] = int(_tmp[1])\n",
    "\n",
    "    return dic_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_bands(dataset_raster):\n",
    "    \"\"\"\n",
    "    Resize bands to correspond at 10m spatial resolution.\n",
    "    @params:\n",
    "        dataser_raster   \t- Required  : dataset raster\n",
    "    \"\"\"\n",
    "    gt = dataset_raster.GetGeoTransform()\n",
    "    res_x = int(gt[1])\n",
    "    res_y = int(abs(gt[-1]))\n",
    "\n",
    "    _res_band = None\n",
    "    if res_x != 10 or res_y != 10:\n",
    "        ratio = float(res_x / 10)\n",
    "        to_resample = dataset_raster.GetRasterBand(1).ReadAsArray()\n",
    "        _res_band = np.array(zoom(to_resample, ratio, order=0))\n",
    "    else:\n",
    "        _res_band = np.array(dataset_raster.GetRasterBand(1).ReadAsArray())\n",
    "\n",
    "    return _res_band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(list, sep=';'):\n",
    "    \"\"\"\n",
    "    Convert a list to string using a separator between each element.\n",
    "    @params:\n",
    "        list   \t- Required  : list to convert\n",
    "        sep     - Required  : separator to put between each element\n",
    "    \"\"\"\n",
    "    res = None\n",
    "\n",
    "    for e in list:\n",
    "        if res is None:\n",
    "            res = str(e)\n",
    "        else:\n",
    "            res += sep + str(e)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel-2 processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sentinel2(tile, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Download every Sentinel-2 L2A images between two dates.\n",
    "    @params:\n",
    "        tile   \t                    - Required  : tilename\n",
    "        start_date                  - Required  : start date\n",
    "        end_date                    - Required  : end date\n",
    "    \"\"\"\n",
    "    command = \"python ./sentinel_download/theia_download.py -t T\" + str(\n",
    "        tile) + \" -c SENTINEL2 -a ./sentinel_download/config_theia.cfg + -d \" + str(start_date) + \" -f \" + str(\n",
    "        end_date) + \" -m 10\"\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCSGE2 processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_projection(path_raster_reference, path_shapefile_to_raster, attribute_shapefile_n5, attribute_shapefile_n4, output_filename,\n",
    "                      attribute_values_n5=None, attribute_values_n4=None):\n",
    "    \"\"\"\n",
    "    Changing shapefile projection to match with raster.\n",
    "    @params:\n",
    "        path_raster_reference   \t- Required  : raster or satellite image as reference\n",
    "        path_shapefile_to_raster   \t- Required  : shapefile to rasterize\n",
    "        attribute_shapefile   \t\t- Required  : attribute to rasterize (name of shapefile column)\n",
    "        output_filename   \t\t\t- Required  : output file rasterized\n",
    "        attribute_values   \t\t\t- Required  : list of attributes to rasterize\n",
    "    \"\"\"\n",
    "    if attribute_values_n4 is None:\n",
    "        attribute_values_n4 = []\n",
    "    if attribute_values_n5 is None:\n",
    "        attribute_values_n5 = []\n",
    "    if (len(attribute_values_n5) == 0):\n",
    "        raise Exception('Error : length of attributes array is 0')\n",
    "\n",
    "    tif = gdal.Open(path_raster_reference)\n",
    "    driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "\n",
    "    dataSource = driver.Open(path_shapefile_to_raster, 0)\n",
    "\n",
    "    layer = dataSource.GetLayer()\n",
    "\n",
    "    sourceprj = layer.GetSpatialRef()\n",
    "    targetprj = osr.SpatialReference(wkt=tif.GetProjection())\n",
    "    transform = osr.CoordinateTransformation(sourceprj, targetprj)\n",
    "\n",
    "    to_fill = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "    ds = to_fill.CreateDataSource(output_filename)\n",
    "    outlayer = ds.CreateLayer('', targetprj, ogr.wkbPolygon)\n",
    "    outlayer.CreateField(ogr.FieldDefn('ID', ogr.OFTInteger))\n",
    "    outlayer.CreateField(ogr.FieldDefn(attribute_shapefile_n5, ogr.OFTInteger))\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for feature in layer:\n",
    "        _val = None\n",
    "\n",
    "        if (feature.GetField(attribute_shapefile_n5) is not None) and (feature.GetField(attribute_shapefile_n5) != 0) :\n",
    "            if int(feature.GetField(attribute_shapefile_n5)) in attribute_values_n5:\n",
    "                _val = feature.GetField(attribute_shapefile_n5)\n",
    "            else:\n",
    "                _val = 0\n",
    "            transformed = feature.GetGeometryRef()\n",
    "            transformed.Transform(transform)\n",
    "\n",
    "            geom = ogr.CreateGeometryFromWkb(transformed.ExportToWkb())\n",
    "            defn = outlayer.GetLayerDefn()\n",
    "            feat = ogr.Feature(defn)\n",
    "            feat.SetField('ID', i)\n",
    "            feat.SetField(attribute_shapefile_n5, _val)\n",
    "            feat.SetGeometry(geom)\n",
    "            outlayer.CreateFeature(feat)\n",
    "            i += 1\n",
    "            feat = None\n",
    "        else:\n",
    "            if int(feature.GetField(attribute_shapefile_n4)) in attribute_values_n4:\n",
    "                _val = feature.GetField(attribute_shapefile_n4)\n",
    "            else:\n",
    "                _val = 0\n",
    "            transformed = feature.GetGeometryRef()\n",
    "            transformed.Transform(transform)\n",
    "\n",
    "            geom = ogr.CreateGeometryFromWkb(transformed.ExportToWkb())\n",
    "            defn = outlayer.GetLayerDefn()\n",
    "            feat = ogr.Feature(defn)\n",
    "            feat.SetField('ID', i)\n",
    "            feat.SetField(attribute_shapefile_n5, _val)\n",
    "            feat.SetGeometry(geom)\n",
    "            outlayer.CreateFeature(feat)\n",
    "            i += 1\n",
    "            feat = None\n",
    "\n",
    "    ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rasterize_ground_ref(path_raster_reference, path_shapefile_to_raster, attribute_shapefile, output_filename):\n",
    "    \"\"\"\n",
    "    Rasterizing ground truth.\n",
    "    @params:\n",
    "        path_raster_reference   \t- Required  : raster or satellite image as reference\n",
    "        path_shapefile_to_raster   \t- Required  : shapefile to rasterize\n",
    "        attribute_shapefile   \t\t- Required  : attribute to rasterize (name of shapefile column)\n",
    "        output_filename   \t\t\t- Required  : output file rasterized\n",
    "    \"\"\"\n",
    "    dataset_raster = gdal.Open(path_raster_reference, GA_ReadOnly)\n",
    "    geo_transform = dataset_raster.GetGeoTransform()\n",
    "\n",
    "    dataset_shapefile = ogr.Open(path_shapefile_to_raster)\n",
    "    layer_shapefile = dataset_shapefile.GetLayer()\n",
    "\n",
    "    nodata_value = 255\n",
    "    # source_layer = data.GetLayer()\n",
    "\n",
    "    x_min = geo_transform[0]\n",
    "    y_max = geo_transform[3]\n",
    "    x_max = x_min + geo_transform[1] * dataset_raster.RasterXSize\n",
    "    y_min = y_max + geo_transform[5] * dataset_raster.RasterYSize\n",
    "    x_res = dataset_raster.RasterXSize\n",
    "    y_res = dataset_raster.RasterYSize\n",
    "    pixel_width = geo_transform[1]\n",
    "\n",
    "    target_ds = gdal.GetDriverByName('GTiff').Create(output_filename, x_res, y_res, 1, gdal.GDT_UInt16)\n",
    "    target_ds.SetGeoTransform(geo_transform)\n",
    "    target_ds.SetProjection(dataset_raster.GetProjection())\n",
    "    band = target_ds.GetRasterBand(1)\n",
    "    band.SetNoDataValue(nodata_value)\n",
    "    band.FlushCache()\n",
    "    band.Fill(nodata_value)\n",
    "\n",
    "    gdal.RasterizeLayer(target_ds, [1], layer_shapefile, options=[\"ATTRIBUTE=\" + str(attribute_shapefile)])\n",
    "\n",
    "    target_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_classes(input_raster, dic_classes, output_filename):\n",
    "    \"\"\"\n",
    "    Merging some classes togerther to get a new typology\n",
    "    @params:\n",
    "        input_raster   \t- Required  : raster to merge\n",
    "        dic_classes   \t- Required  : dictionnary containing original classes as keys, and new typologies as value\n",
    "        output_filename - Required  : output path to create the new raster\n",
    "    \"\"\"\n",
    "    ds_raster = gdal.Open(input_raster, GA_ReadOnly)\n",
    "    array_raster = ds_raster.GetRasterBand(1).ReadAsArray()\n",
    "    tab_return = None\n",
    "\n",
    "    for key in dic_classes:\n",
    "        if tab_return is None:\n",
    "            tab_return = np.where(array_raster == key, dic_classes[key], 0)\n",
    "        else:\n",
    "            tab_return += np.where(array_raster == key, dic_classes[key], 0)\n",
    "\n",
    "    if -99 in tab_return:\n",
    "        tab_return = ma.masked_array(tab_return, tab_return == -99)\n",
    "        for shift in (-3, 3):\n",
    "            for axis in (0, 1):\n",
    "                a_shifted = np.roll(tab_return, shift=shift, axis=axis)\n",
    "                idx = ~a_shifted.mask * tab_return.mask\n",
    "                tab_return[idx] = a_shifted[idx]\n",
    "\n",
    "    '''se_disk = morphology.disk(3)\n",
    "    se_rectangle = morphology.rectangle(3, 3)\n",
    "    tab_return = morphology.opening(tab_return, se_disk)\n",
    "    tab_return = morphology.opening(tab_return, se_rectangle)'''\n",
    "\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    dst_ds = driver.Create(output_filename, ds_raster.RasterXSize, ds_raster.RasterYSize, 1, gdal.GDT_Byte)\n",
    "    dst_ds.SetProjection(ds_raster.GetProjection())\n",
    "    geotransform = ds_raster.GetGeoTransform()\n",
    "    dst_ds.SetGeoTransform(geotransform)\n",
    "    dst_ds.GetRasterBand(1).WriteArray(tab_return)\n",
    "    dst_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computing_roads(output_folder, shapefile_roads, roads_pix_value, output_merging_tif, output_name_with_roads):\n",
    "    \"\"\"\n",
    "    Adding roads to the ground reference rasters\n",
    "    @params:\n",
    "        output_folder   \t- Required  : raster to merge\n",
    "        shapefile_roads   \t- Required  : dictionnary containing original classes as keys, and new typologies as value\n",
    "        roads_pix_value     - Required  : output path to create the new raster\n",
    "        output_merging_tif  - Required  : output path to create the new raster\n",
    "        output_name_with_roads  - Required  : output path to create the new raster\n",
    "    \"\"\"\n",
    "    output_filename_reproj = os.path.join(output_folder, 'roads_reproj.shp')\n",
    "    change_projection(output_merging_tif, shapefile_roads, 'VAL', None, output_filename_reproj, [99], None)\n",
    "\n",
    "    output_filename_rasterize = os.path.join(output_folder, 'roads_reproj.tif')\n",
    "    rasterize_ground_ref(output_merging_tif, output_filename_reproj, 'VAL', output_filename_rasterize)\n",
    "\n",
    "    dataset_roads = gdal.Open(output_filename_rasterize, GA_ReadOnly)\n",
    "    dataset_gt = gdal.Open(output_merging_tif, GA_ReadOnly)\n",
    "\n",
    "    band_roads = dataset_roads.GetRasterBand(1).ReadAsArray()\n",
    "    band_gt = dataset_gt.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "    band_roads[band_roads == 255] = 0\n",
    "    _tmp = band_gt + band_roads\n",
    "\n",
    "    _tmp[_tmp >= 90] = roads_pix_value\n",
    "    _tmp[_tmp == 25] = 5\n",
    "\n",
    "    shape = _tmp.shape\n",
    "\n",
    "    output = output_name_with_roads\n",
    "\n",
    "    geo_transform = dataset_gt.GetGeoTransform()\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    dst_ds = driver.Create(output, shape[1], shape[0], 1, gdal.GDT_UInt16)\n",
    "    dst_ds.SetProjection(dataset_gt.GetProjection())\n",
    "    dst_ds.SetGeoTransform(geo_transform)\n",
    "    dst_ds.GetRasterBand(1).WriteArray(_tmp)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connected_components(path_ground_reference, output_filename, min_size=400):\n",
    "    dataset_gt = gdal.Open(path_ground_reference, GA_ReadOnly)\n",
    "    gr = dataset_gt.GetRasterBand(1).ReadAsArray()\n",
    "    #gr = gr[0:1000,0:1000]\n",
    "    \n",
    "    max_gr = np.amax(gr)\n",
    "    res = None\n",
    "    mask = np.uint8(np.where(gr == 0, 255, 0))\n",
    "    \n",
    "    for i in range(1, max_gr + 1):\n",
    "        tmp_array_1 = np.uint8(np.where(gr == i, 127, 255))\n",
    "        tmp_array_1 = cv2.threshold(tmp_array_1, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "        \n",
    "        nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(tmp_array_1, connectivity=8)\n",
    "        sizes = stats[1:, -1]; nb_components = nb_components - 1\n",
    "        img1 = np.zeros((output.shape), dtype=int)\n",
    "\n",
    "        for j in range(0, nb_components):\n",
    "            if sizes[j] >= min_size:\n",
    "                img1[output == j + 1] = 255\n",
    "        \n",
    "        img1 = np.uint8(np.where(img1 == 0, 255, 127))\n",
    "        tmp_array_2 = cv2.threshold(img1, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "                \n",
    "        nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(tmp_array_2, connectivity=8)\n",
    "        sizes = stats[1:, -1]; nb_components = nb_components - 1\n",
    "        img2 = np.zeros((output.shape), dtype=int)\n",
    "\n",
    "        for k in range(0, nb_components):\n",
    "            if sizes[k] >= min_size:\n",
    "                img2[output == k + 1] = 255\n",
    "        \n",
    "        if res is None:\n",
    "            res = np.zeros((img2.shape), dtype=int)\n",
    "        \n",
    "        img2 = np.where(img2 == 0, 0, i)\n",
    "    \n",
    "        '''geo_transform = dataset_gt.GetGeoTransform()\n",
    "        driver = gdal.GetDriverByName(\"GTiff\")\n",
    "        dst_ds = driver.Create('/media/wenger/LaCie/create_dataset/tmp/' + str(i) + '_img2.tif', res.shape[1], res.shape[0], 1, gdal.GDT_UInt16, options=[\"COMPRESS=LZW\"])\n",
    "        dst_ds.SetProjection(dataset_gt.GetProjection())\n",
    "        dst_ds.SetGeoTransform(geo_transform)\n",
    "        dst_ds.GetRasterBand(1).WriteArray(img2)'''\n",
    "        \n",
    "        res += img2\n",
    "        \n",
    "        '''mask_res_tmp = np.where(img1 != 0, 1, 0)\n",
    "        res *= mask_res_tmp\n",
    "        res += np.where(img1 == 0, i, 0)\n",
    "        res += mask\n",
    "        res = np.where(res >= 255, 255, res)'''\n",
    "    \n",
    "    res += mask\n",
    "    res = np.uint8(np.where(res >= 255, 255, res))\n",
    "    \n",
    "    #imputer = SimpleImputer(missing_values=0, strategy='mean')\n",
    "    imputer = KNNImputer(missing_values=0, n_neighbors=1)\n",
    "    #to_mask = np.uint8(np.where(res == 0, np.nan, res))\n",
    "    dst = imputer.fit_transform(res)\n",
    "    \n",
    "    geo_transform = dataset_gt.GetGeoTransform()\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    dst_ds = driver.Create(output_filename, res.shape[1], res.shape[0], 1, gdal.GDT_UInt16, options=[\"COMPRESS=LZW\"])\n",
    "    dst_ds.SetProjection(dataset_gt.GetProjection())\n",
    "    dst_ds.SetGeoTransform(geo_transform)\n",
    "    dst_ds.GetRasterBand(1).WriteArray(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gr = '/media/wenger/LaCie/create_dataset/ground_reference_raster/32ULU_ground_reference.tif'\n",
    "tmp = '/media/wenger/LaCie/create_dataset/tmp/32ULU_test.tif'\n",
    "connected_components(path_gr, tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel-1 processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sentinel1(tiles_list, directory_images, step=8):\n",
    "    \"\"\"\n",
    "    Download every Sentinel-2 L2A images between two dates.\n",
    "    @params:\n",
    "        tiles_list   \t        - Required  : list of Sentinel-2 tiles\n",
    "        directory_images        - Required  : directory containing Sentinel-2 images\n",
    "        step                    - Optional  : day interval between Sentinel-2 image to download Sentinel-1 images\n",
    "    \"\"\"\n",
    "    s1_output = './S1_img'\n",
    "    for tile in tiles_list:\n",
    "        s1_tile = tile\n",
    "\n",
    "        for img in os.listdir(directory_images):\n",
    "            if s1_tile in img:\n",
    "                _date = get_date_from_image_path(img)\n",
    "                datetime_object = datetime.strptime(_date, '%Y%m%d')\n",
    "                s1_first_date = (datetime_object - timedelta(days=int(step))).strftime('%Y-%m-%d')\n",
    "                s1_last_date = (datetime_object + timedelta(days=int(step))).strftime('%Y-%m-%d')\n",
    "\n",
    "                replacements = {'{output_path}': s1_output, '{first_date}': s1_first_date, '{last_date}': s1_last_date,\n",
    "                                '{tiles}': s1_tile}\n",
    "\n",
    "                with open('./sentinel_1_tiling/S1Processor.cfg') as infile, open(\n",
    "                        './sentinel_1_tiling/S1Processor_tmp.cfg', 'w') as outfile:\n",
    "                    for line in infile:\n",
    "                        for src, target in replacements.iteritems():\n",
    "                            line = line.replace(src, target)\n",
    "                        outfile.write(line)\n",
    "\n",
    "                command = 'python ./sentinel_1_tiling/S1Processor.py ./sentinel_1_tiling/S1Processor_tmp.cfg'\n",
    "                os.system(command)\n",
    "                os.system('rm ./sentinel_1_tiling/S1Processor_tmp.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_satellite_data(tiles, directory_s2, directory_s1, step=8, filtered=True):\n",
    "    \"\"\"\n",
    "    Construct JSON file to create database of S1 and S2 images.\n",
    "    @params:\n",
    "        tiles   \t        - Required  : list of Sentinel-2 tiles\n",
    "        directory_s2        - Required  : directory containing Sentinel-2 images\n",
    "        directory_s1        - Required  : directory containing Sentinel-1 images pretreated from s1tiling\n",
    "        step                - Optional  : day interval between Sentinel-2 image to download Sentinel-1 images\n",
    "        filtered            - Optional  : using or not filtered images\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "\n",
    "    for tile in tiles:\n",
    "        data[tile] = []\n",
    "        for img_s2 in os.listdir(directory_s2):\n",
    "            if tile in img_s2:\n",
    "                date = get_date_from_image_path(img_s2)\n",
    "                s1_path_vv = get_s1_from_s2_tile(tile, date, directory_s1, 'vv', step=step, filtered=filtered)\n",
    "                s1_path_vh = get_s1_from_s2_tile(tile, date, directory_s1, 'vh', step=step, filtered=filtered)\n",
    "                data[tile].append({\n",
    "                    'dateS2': date,\n",
    "                    's2_path': os.path.join(directory_s2, img_s2),\n",
    "                    's1_path_vv': s1_path_vv,\n",
    "                    's1_path_vh': s1_path_vh\n",
    "                })\n",
    "\n",
    "    if os.path.exists('./sentinel_data.json'):\n",
    "        os.remove('./sentinel_data.json')\n",
    "\n",
    "    with open('./sentinel_data.json', 'w') as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s1_from_s2_tile(tile, date_s2, directory_s1, pol, step=8, filtered=True):\n",
    "    \"\"\"\n",
    "    Extract filename of the Sentinel-1 image\n",
    "    @params:\n",
    "        tile   \t            - Required  : current tile\n",
    "        date_s2             - Required  : date of current Sentinel-2\n",
    "        directory_s1        - Required  : directory containing Sentinel-1 images pretreated from s1tiling\n",
    "        step                - Optional  : day interval between Sentinel-2 image to download Sentinel-1 images\n",
    "        filtered            - Optional  : using or not filtered images\n",
    "    \"\"\"\n",
    "    path_folder = os.path.join(directory_s1, str(tile))\n",
    "    if filtered:\n",
    "        path_folder = os.path.join(directory_s1, str(tile), 'filtered')\n",
    "\n",
    "    s1_files = []\n",
    "    datetime_s2 = datetime.strptime(date_s2, '%Y%m%d')\n",
    "    s1_first_date = datetime_s2 - timedelta(days=int(step))\n",
    "    s1_last_date = datetime_s2 + timedelta(days=int(step))\n",
    "\n",
    "    for file in os.listdir(path_folder):\n",
    "        if (tile in file) and (pol in file):\n",
    "            datetime_current_s1 = datetime.strptime(get_date_from_image_path(file, s1=True), '%Y%m%d')\n",
    "            if s1_first_date <= datetime_current_s1 <= s1_last_date:\n",
    "                s1_files.append(file)\n",
    "\n",
    "    _size = None\n",
    "    res_img = None\n",
    "    for file in s1_files:\n",
    "        _img = tiff.imread(os.path.join(path_folder, file))\n",
    "        if _size is None:\n",
    "            _size = np.count_nonzero(_img == 0)\n",
    "            res_img = file\n",
    "        else:\n",
    "            if np.count_nonzero(_img == 0) < _size:\n",
    "                res_img = file\n",
    "\n",
    "    return os.path.join(path_folder, res_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_image_path(name, s1=False):\n",
    "    \"\"\"\n",
    "    Return date from Sentinel-2 folder name or Sentinel-1 pretreated name.\n",
    "    @params:\n",
    "        path   \t                    - Required  : Folder name containing every image bands.\n",
    "        s1   \t                    - Optional  : True if name is Sentinel-1 filename\n",
    "    \"\"\"\n",
    "    if s1:\n",
    "        date = name.split('_')[5][0:8]\n",
    "    else:\n",
    "        date = name.split('_')[1].split('-')[0]\n",
    "\n",
    "    return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders_dataset(path_directory_dataset):\n",
    "    if os.path.exists(os.path.join(path_directory_dataset, 's1')):\n",
    "        shutil.rmtree(os.path.join(path_directory_dataset, 's1'))\n",
    "        os.mkdir(os.path.join(path_directory_dataset, 's1'))\n",
    "    else:\n",
    "        os.mkdir(os.path.join(path_directory_dataset, 's1'))\n",
    "\n",
    "    if os.path.exists(os.path.join(path_directory_dataset, 's2')):\n",
    "        shutil.rmtree(os.path.join(path_directory_dataset, 's2'))\n",
    "        os.mkdir(os.path.join(path_directory_dataset, 's2'))\n",
    "    else:\n",
    "        os.mkdir(os.path.join(path_directory_dataset, 's2'))\n",
    "\n",
    "    if os.path.exists(os.path.join(path_directory_dataset, 'ground_reference')):\n",
    "        shutil.rmtree(os.path.join(path_directory_dataset, 'ground_reference'))\n",
    "        os.mkdir(os.path.join(path_directory_dataset, 'ground_reference'))\n",
    "    else:\n",
    "        os.mkdir(os.path.join(path_directory_dataset, 'ground_reference'))\n",
    "\n",
    "    if os.path.exists(os.path.join(path_directory_dataset, 'labels')):\n",
    "        shutil.rmtree(os.path.join(path_directory_dataset, 'labels'))\n",
    "        os.mkdir(os.path.join(path_directory_dataset, 'labels'))\n",
    "    else:\n",
    "        os.mkdir(os.path.join(path_directory_dataset, 'labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_patches_tile(tile, directory_ground_reference, directory_sat_images_s2, directory_sat_images_s1,\n",
    "                           directory_dataset, patch_size, method=1):\n",
    "    bands_S2 = ['B2.tif', 'B3.tif', 'B4.tif', 'B5.tif', 'B6.tif', 'B7.tif', 'B8.tif', 'B8A.tif', 'B11.tif', 'B12.tif']\n",
    "    polar_S1 = ['vv', 'vh']\n",
    "    path_gr = None\n",
    "\n",
    "    for gr in os.listdir(directory_ground_reference):\n",
    "        if tile in gr:\n",
    "            path_gr = os.path.join(directory_ground_reference, gr)\n",
    "\n",
    "    assert path_gr is not None\n",
    "\n",
    "    ds_gr_raster = gdal.Open(path_gr)\n",
    "    gr_raster = ds_gr_raster.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "    if method == 1:\n",
    "        step = patch_size\n",
    "    else:\n",
    "        step = 1\n",
    "\n",
    "    for y in range(0, gr_raster.shape[0], step):\n",
    "        for x in range(0, gr_raster.shape[1], step):\n",
    "            if method == 2:\n",
    "                step = 1\n",
    "\n",
    "            gr_patch = gr_raster[y:y + patch_size, x:x + patch_size]\n",
    "\n",
    "            if 0 not in gr_patch:\n",
    "                if method == 2:\n",
    "                    step = patch_size + 1\n",
    "\n",
    "                x_coords_topleft, y_coords_topleft, x_size, y_size = get_coordinates(ds_gr_raster,\n",
    "                                                                                     x, y)\n",
    "                new_geotrans = (x_coords_topleft, x_size, 0, y_coords_topleft, 0, y_size)\n",
    "\n",
    "                driver = gdal.GetDriverByName(\"GTiff\")\n",
    "\n",
    "                for img_dir in os.listdir(directory_sat_images_s2):\n",
    "                    if tile in img_dir:\n",
    "                        date_s2 = get_date_from_image_path(img_dir)\n",
    "                        output_patch_name_s2 = os.path.join(directory_dataset, 's2', str(tile) + '_' + str(date_s2) + '_S2_' +\n",
    "                                                            str(x) + '_' + str(y) + '.tif')\n",
    "                        output_patch_name_gr = os.path.join(directory_dataset, 'ground_reference', str(tile) + '_GR_' +\n",
    "                                                            str(x) + '_' + str(y) + '.tif')\n",
    "\n",
    "                        dst_ds_img_s2 = driver.Create(output_patch_name_s2, patch_size, patch_size, len(bands_S2),\n",
    "                                                      gdal.GDT_Float32, options=[\"COMPRESS=LZW\"])\n",
    "                        dst_ds_img_s2.SetProjection(ds_gr_raster.GetProjection())\n",
    "                        dst_ds_img_s2.SetGeoTransform(new_geotrans)\n",
    "\n",
    "                        if not os.path.exists(output_patch_name_gr):\n",
    "                            dst_ds_mask = driver.Create(output_patch_name_gr, patch_size, patch_size, 1, gdal.GDT_UInt16,\n",
    "                                                    options=[\"COMPRESS=LZW\"])\n",
    "                            dst_ds_mask.SetProjection(ds_gr_raster.GetProjection())\n",
    "                            dst_ds_mask.SetGeoTransform(new_geotrans)\n",
    "                            dst_ds_mask.GetRasterBand(1).WriteArray(gr_patch)\n",
    "\n",
    "                        count_s2_bands = 1\n",
    "                        for band_name in bands_S2:\n",
    "                            for band in os.listdir(os.path.join(directory_sat_images_s2, img_dir)):\n",
    "                                if ('SRE' in band) and (band_name in band):\n",
    "                                    ds_band = gdal.Open(os.path.join(directory_sat_images_s2, img_dir, band))\n",
    "                                    patch_band = resize_bands(ds_band)[y:y + patch_size, x:x + patch_size]\n",
    "                                    dst_ds_img_s2.GetRasterBand(count_s2_bands).WriteArray(patch_band)\n",
    "                                    count_s2_bands += 1\n",
    "\n",
    "                for folder_tile in os.listdir(directory_sat_images_s1):\n",
    "                    if str(folder_tile) == str(tile):\n",
    "                        for year in os.listdir(os.path.join(directory_sat_images_s1, folder_tile)):\n",
    "                            for month in os.listdir(os.path.join(directory_sat_images_s1, folder_tile, year)):\n",
    "                                complete_dates_s1 = get_s1_dates(os.path.join(directory_sat_images_s1, folder_tile,\n",
    "                                                                              year, month))\n",
    "\n",
    "                                for date in complete_dates_s1:\n",
    "                                    count_s1_polar = 1\n",
    "                                    date_s1 = str(year) + str(month) + str(date[-2:])\n",
    "                                    output_patch_name_s1 = os.path.join(directory_dataset, 's1',\n",
    "                                                                        str(tile) + '_' + str(date_s1) + '_S1_' +\n",
    "                                                                        str(x) + '_' + str(y) + '.tif')\n",
    "                                    dst_ds_img_s1 = driver.Create(output_patch_name_s1, patch_size, patch_size,\n",
    "                                                                  len(polar_S1),\n",
    "                                                                  gdal.GDT_Float32, options=[\"COMPRESS=LZW\"])\n",
    "                                    dst_ds_img_s1.SetProjection(ds_gr_raster.GetProjection())\n",
    "                                    dst_ds_img_s1.SetGeoTransform(new_geotrans)\n",
    "\n",
    "                                    for polar in polar_S1:\n",
    "                                        for img in os.listdir(\n",
    "                                                os.path.join(directory_sat_images_s1, folder_tile, year, month)):\n",
    "                                            if (polar in img) and (date in img):\n",
    "                                                path_radar = os.path.join(directory_sat_images_s1, folder_tile, year,\n",
    "                                                                          month, img)\n",
    "                                                ds_band = gdal.Open(path_radar)\n",
    "                                                patch_band = resize_bands(ds_band)[y:y + patch_size, x:x + patch_size]\n",
    "                                                dst_ds_img_s1.GetRasterBand(count_s1_polar).WriteArray(patch_band)\n",
    "                                                count_s1_polar += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_labels(path_dataset):\n",
    "    \"\"\"\n",
    "    Construct JSON file to generate labels for scene classification.\n",
    "    @params:\n",
    "        path_ground_reference   \t- Required  : path to ground reference folder\n",
    "        path_output_json            - Required  : path to the json directory\n",
    "    \"\"\"\n",
    "    for file in os.listdir(os.path.join(path_dataset, 'ground_reference')):\n",
    "        data = {}\n",
    "        patch_infos = extract_info_patches(os.path.basename(file), is_gr=True)\n",
    "        data['corresponding_s2'] = get_corresponding_sat(os.path.join(path_dataset, 's2'), patch_infos[0], patch_infos[1],\n",
    "                                                         patch_infos[2])\n",
    "        data['corresponding_s1'] = get_corresponding_sat(os.path.join(path_dataset, 's1'), patch_infos[0], patch_infos[1],\n",
    "                                                         patch_infos[2])\n",
    "        data['projection'] = get_projection_patch(os.path.join(path_dataset, 'ground_reference', file))\n",
    "        data['labels'] = list_to_string(extract_classes_gr(os.path.join(path_dataset, 'ground_reference', file)).keys())\n",
    "\n",
    "        json_name = patch_infos[0] + '_' + patch_infos[1] + '_' + patch_infos[2] + '.json'\n",
    "\n",
    "        if os.path.exists(os.path.join(path_dataset, 'labels', json_name)):\n",
    "            os.remove(os.path.join(path_dataset, 'labels', json_name))\n",
    "\n",
    "        with open(os.path.join(path_dataset, 'labels', json_name), 'w') as file:\n",
    "            json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classes_gr(path_gr_patch):\n",
    "    \"\"\"\n",
    "    Extract classes in ground reference patch.\n",
    "    @params:\n",
    "        path_gr_patch   \t- Required  : filename of the patch\n",
    "    \"\"\"\n",
    "    dataset = gdal.Open(path_gr_patch, GA_ReadOnly)\n",
    "    array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "    unique, counts = np.unique(array, return_counts=True)\n",
    "\n",
    "    return dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corresponding_sat(path_folder, tile, tile_x, tile_y):\n",
    "    \"\"\"\n",
    "    Extract every file in the folder corresponding to the coordinates and concatenate them in one single string.\n",
    "    @params:\n",
    "        path_folder   \t- Required  : filename of the patch\n",
    "        tile_x          - Required  : boolean to tell if the filename is ground reference or not\n",
    "        tile_y          - Required  : boolean to tell if the filename is ground reference or not\n",
    "    \"\"\"\n",
    "    output = None\n",
    "\n",
    "    for file in os.listdir(path_folder):\n",
    "        string = tile_x + '_' + tile_y\n",
    "        if (string in file) and (tile in file):\n",
    "            if output is None:\n",
    "                output = file\n",
    "            else:\n",
    "                output += ';' + file\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s1_dates(path):\n",
    "    res = []\n",
    "\n",
    "    assert len(os.listdir(path)) > 2\n",
    "\n",
    "    for img in os.listdir(path):\n",
    "        _c = img.split('_')[5][0:8]\n",
    "        if _c not in res:\n",
    "            res.append(_c)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_patches(patch_name, is_gr=False):\n",
    "    \"\"\"\n",
    "    Extract informations from patch name.\n",
    "    @params:\n",
    "        patch_name   \t- Required  : filename of the patch\n",
    "        is_gr           - Required  : boolean to tell if the filename is ground reference or not\n",
    "    \"\"\"\n",
    "    split = patch_name.split('_')\n",
    "\n",
    "    if is_gr:\n",
    "        tile = split[0]\n",
    "        tile_x = split[2]\n",
    "        tile_y = split[3].split('.')[0]\n",
    "        output = (tile, tile_x, tile_y)\n",
    "    else:\n",
    "        tile = split[0]\n",
    "        date_img = split[1]\n",
    "        sat = split[2]\n",
    "        tile_x = split[3]\n",
    "        tile_y = split[4]\n",
    "        output = (tile, date_img, sat, tile_x, tile_y)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(dataset, x_index_topleft, y_index_topleft):\n",
    "    \"\"\"\n",
    "    Get upper left coordinates using matrix cell number\n",
    "    @params:\n",
    "        dataset   \t\t  - Required  : dataset raster\n",
    "        x_index_topleft   - Required  : x cell index\n",
    "        y_index_topleft   - Required  : y cell index\n",
    "    \"\"\"\n",
    "    if dataset is not None:\n",
    "        (upper_left_x, x_size, x_rotation, upper_left_y, y_rotation, y_size) = dataset.GetGeoTransform()\n",
    "\n",
    "        x_coords_topleft = x_index_topleft * x_size + upper_left_x\n",
    "        y_coords_topleft = y_index_topleft * y_size + upper_left_y\n",
    "\n",
    "        return (x_coords_topleft, y_coords_topleft, x_size, y_size)\n",
    "    else:\n",
    "        raise Exception('Error : dataset is None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projection_patch(path_patch):\n",
    "    \"\"\"\n",
    "    Get the projection (WKT) of a patch.\n",
    "    @params:\n",
    "        path_patch   \t- Required  : path to the patch\n",
    "    \"\"\"\n",
    "    dataset = gdal.Open(path_patch, GA_ReadOnly)\n",
    "    proj = dataset.GetProjection()\n",
    "\n",
    "    return proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking overlap between patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patch:\n",
    "    x = 0\n",
    "    y = 0\n",
    "    tile = None\n",
    "    \n",
    "    def __init__(self, x, y, tile):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.tile = tile\n",
    "    \n",
    "    def affiche(self):\n",
    "        print(\"x =\", self.x, \"y =\", self.y, \"tile =\", self.tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_overlapping_patches(directory_dataset):\n",
    "    \"\"\"\n",
    "    Finding an overlap between two rasters. This function works even if rasters have different projection.\n",
    "    @params:\n",
    "        directory_dataset   \t- Required  : path to the first raster\n",
    "    \"\"\"\n",
    "    path_gr = os.path.join(directory_dataset, 'ground_reference')\n",
    "    path_s1 = os.path.join(directory_dataset, 's1')\n",
    "    path_s2 = os.path.join(directory_dataset, 's2')\n",
    "    path_labels = os.path.join(directory_dataset, 'labels')\n",
    "    \n",
    "    dic_res = []\n",
    "    \n",
    "    if os.path.exists(os.path.join(directory_dataset, 'overlapping.txt')):\n",
    "        os.remove(os.path.join(directory_dataset, 'overlapping.txt'))\n",
    "    \n",
    "    for patch1 in os.listdir(path_gr):\n",
    "        tile_patch1 = patch1.split('_')[0]\n",
    "        for patch2 in os.listdir(path_gr):\n",
    "            tile_patch2 = patch2.split('_')[0]\n",
    "            if tile_patch1 != tile_patch2:\n",
    "                if find_raster_intersect(os.path.join(path_gr, patch1), os.path.join(path_gr, patch2)):\n",
    "                    x = patch1.split('_')[2]\n",
    "                    y = patch1.split('_')[3].split('.')[0]\n",
    "\n",
    "                    patch_to_delete = Patch(x, y, tile_patch1)\n",
    "                    dic_res.append(patch_to_delete)\n",
    "                    \n",
    "                    with open(os.path.join(directory_dataset, 'overlapping.txt'), \"a\") as f:\n",
    "                        f.write(str(patch1) + ' overlap ' + str(patch2) + '\\n')\n",
    "    \n",
    "    for patch in dic_res:\n",
    "        delete_every_patch(patch.x, patch.y, patch.tile, directory_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_every_patch(x, y, tile, directory_dataset):\n",
    "    \"\"\"Delete every patch matching x, y and tile\n",
    "    @params:\n",
    "        x   \t- Required  : x coordinates of the patch\n",
    "        y   \t- Required  : y coordinates of the patch\n",
    "        tile   \t- Required  : tile of the patch\n",
    "        directory_dataset   \t- Required  : dataset directory\n",
    "    \"\"\"\n",
    "    path_gr = os.path.join(directory_dataset, 'ground_reference')\n",
    "    path_s1 = os.path.join(directory_dataset, 's1')\n",
    "    path_s2 = os.path.join(directory_dataset, 's2')\n",
    "    path_labels = os.path.join(directory_dataset, 'labels')\n",
    "    \n",
    "    #Delete groud reference patch\n",
    "    for f in os.listdir(os.path.join(directory_dataset, path_gr)):\n",
    "        tile_f = f.split('_')[0]\n",
    "        x_f = f.split('_')[2]\n",
    "        y_f = f.split('_')[3].split('.')[0]\n",
    "        \n",
    "        if (str(x) == str(x_f)) and (str(y_f) == str(y)) and (str(tile) == str(tile_f)):\n",
    "            os.remove(os.path.join(directory_dataset, path_gr, f))\n",
    "            \n",
    "    #Delete S1 patch\n",
    "    for f in os.listdir(os.path.join(directory_dataset, path_s1)):\n",
    "        tile_f = f.split('_')[0]\n",
    "        x_f = f.split('_')[3]\n",
    "        y_f = f.split('_')[4].split('.')[0]\n",
    "        \n",
    "        if (str(x) == str(x_f)) and (str(y_f) == str(y)) and (str(tile) == str(tile_f)):\n",
    "            os.remove(os.path.join(directory_dataset, path_s1, f))\n",
    "    \n",
    "    #Delete S2 patch\n",
    "    for f in os.listdir(os.path.join(directory_dataset, path_s2)):\n",
    "        tile_f = f.split('_')[0]\n",
    "        x_f = f.split('_')[3]\n",
    "        y_f = f.split('_')[4].split('.')[0]\n",
    "        \n",
    "        if (str(x) == str(x_f)) and (str(y_f) == str(y)) and (str(tile) == str(tile_f)):\n",
    "            os.remove(os.path.join(directory_dataset, path_s2, f))\n",
    "            \n",
    "    #Delete labels patch\n",
    "    for f in os.listdir(os.path.join(directory_dataset, path_labels)):\n",
    "        tile_f = f.split('_')[0]\n",
    "        x_f = f.split('_')[1]\n",
    "        y_f = f.split('_')[2].split('.')[0]\n",
    "        \n",
    "        if (str(x) == str(x_f)) and (str(y_f) == str(y)) and (str(tile) == str(tile_f)):\n",
    "            os.remove(os.path.join(directory_dataset, path_labels, f))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_raster_intersect(path_raster1, path_raster2, with_text=False):\n",
    "    \"\"\"\n",
    "    Finding an overlap between two rasters. This function works even if rasters have different projection.\n",
    "    @params:\n",
    "        path_raster1   \t- Required  : path to the first raster\n",
    "        path_raster2   \t- Required  : path to the second raster\n",
    "    \"\"\"\n",
    "    overlap = False\n",
    "    \n",
    "    raster1 = gdal.Open(path_raster1)\n",
    "    raster2 = gdal.Open(path_raster2)\n",
    "    gt1 = raster1.GetGeoTransform()\n",
    "    gt2 = raster2.GetGeoTransform()\n",
    "    \n",
    "    proj_raster1 = osr.SpatialReference(wkt=raster1.GetProjection())\n",
    "    epsg_raster1 = proj_raster1.GetAttrValue('AUTHORITY',1)\n",
    "    proj_raster2 = osr.SpatialReference(wkt=raster2.GetProjection())\n",
    "    epsg_raster2 = proj_raster2.GetAttrValue('AUTHORITY',1)\n",
    "\n",
    "    # find each image's bounding box\n",
    "    r1 = [gt1[0], gt1[3], gt1[0] + (gt1[1] * raster1.RasterXSize), gt1[3] + (gt1[5] * raster1.RasterYSize)]\n",
    "    r1_x1, r1_y1 = change_coordinates(r1[0], r1[1], epsg_raster1)\n",
    "    r1_x2, r1_y2 = change_coordinates(r1[2], r1[3], epsg_raster1)\n",
    "    r1 = [r1_x1, r1_y1, r1_x2, r1_y2]\n",
    "    \n",
    "    r2 = [gt2[0], gt2[3], gt2[0] + (gt2[1] * raster2.RasterXSize), gt2[3] + (gt2[5] * raster2.RasterYSize)]\n",
    "    r2_x1, r2_y1 = change_coordinates(r2[0], r2[1], epsg_raster2)\n",
    "    r2_x2, r2_y2 = change_coordinates(r2[2], r2[3], epsg_raster2)\n",
    "    r2 = [r2_x1, r2_y1, r2_x2, r2_y2]\n",
    "    \n",
    "    if with_text:\n",
    "        print('\\t1 bounding box: %s' % str(r1))\n",
    "        print('\\t2 bounding box: %s' % str(r2))\n",
    "\n",
    "    # find intersection between bounding boxes\n",
    "    intersection = [max(r1[0], r2[0]), min(r1[1], r2[1]), min(r1[2], r2[2]), max(r1[3], r2[3])]\n",
    "    if r1 != r2:\n",
    "        # check for any overlap at all...\n",
    "        if (intersection[2] < intersection[0]) or (intersection[1] < intersection[3]):\n",
    "            overlap = False\n",
    "            if with_text:\n",
    "                print('\\tThat\\'s not an overlap !')\n",
    "        else:\n",
    "            overlap = True\n",
    "            if with_text:\n",
    "                print('\\tThat\\'s an overlap !')\n",
    "\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_coordinates(x, y, epsg_from, epsg_to='4326'):\n",
    "    \"\"\"\n",
    "    Changing coordinates from an epsg to an other.\n",
    "    @params:\n",
    "        x   \t        - Required  : x coordinate\n",
    "        y   \t        - Required  : y coordinate\n",
    "        epsg_from   \t- Required  : epsg of x and y coordinates\n",
    "        epsg_to   \t    - Required  : epsg where they need to be converted\n",
    "    \"\"\"\n",
    "    inProj = Proj(init='epsg:' + epsg_from)\n",
    "    outProj = Proj(init='epsg:' + epsg_to)\n",
    "    x2,y2 = transform(inProj,outProj,x,y)\n",
    "    \n",
    "    return x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_zero_patch(directory_dataset):\n",
    "    \"\"\"\n",
    "    Deleting patches where ground reference contains zeros.\n",
    "    @params:\n",
    "        directory_dataset   \t- Required  : path to the first raster\n",
    "    \"\"\"\n",
    "    path_gr = os.path.join(directory_dataset, 'ground_reference')\n",
    "    path_s1 = os.path.join(directory_dataset, 's1')\n",
    "    path_s2 = os.path.join(directory_dataset, 's2')\n",
    "    path_labels = os.path.join(directory_dataset, 'labels')\n",
    "    \n",
    "    for patch in os.listdir(path_gr):\n",
    "        raster = gdal.Open(os.path.join(path_gr, patch))\n",
    "        array = raster.GetRasterBand(1).ReadAsArray()\n",
    "        \n",
    "        if 0 in array:\n",
    "            tile = patch.split('_')[0]\n",
    "            x = patch.split('_')[2]\n",
    "            y = patch.split('_')[3].split('.')[0]\n",
    "            delete_every_patch(x, y, tile, directory_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs\n",
    "\n",
    "This part regroups all the codes enabling the preprocessing of the data and the creation of the dataset. It is organized in the same way as the previous functions with a sub-part per treatment.\n",
    "\n",
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "# TODO: Regroupement des classes\n",
    "urban_regroup_classes = '11111:1 11112:1 11113:1 11121:1 11122:1 11123:1 11211:2 11212:2 11213:2 11221:2 11222:2 11223:2 11231:2 11232:2 11233:2 11241:2 11242:2 11243:2 11301:2 11302:2 11303:2 11401:3 11402:3 11403:3 12111:3 12112:3 12113:3 12121:3 12122:3 12123:3 12131:3 12132:3 12133:4 13141:3 13142:3 13123:4 12151:3 12152:3 12153:4 12201:3 12202:3 12203:4 13111:3 13112:3 13113:3 13121:3 13122:3 13123:3 13131:3 13132:3 13133:3 13141:3 13142:3 13143:4 13201:1 13202:3 13203:3 13301:2 13302:2 13303:2 13401:3 13402:3 13403:3 14111:3 14112:3 14113:5 14201:3 14202:3 14203:4 14301:3 14302:3 14303:3 15101:3 15102:3 15103:4 16101:3 16102:3 16103:3 17101:1 17102:1 17103:1 12141:4 12142:4 12143:3 14131:5 14132:5 14133:5 14121:-99 14122:-99 14123:-99'\n",
    "natural_regroup_classes = '2110:6 2120:6 2210:7 2221:8 2222:8 2223:8 2310:9 2320:10 3110:11 3120:11 3130:11 3140:11 3150:11 3210:12 3220:12 3230:12 3310:13 3320:13 3340:13 4110:14 4120:14 5110:15 5120:15 5130:15'\n",
    "path_shapefile_sentinel = './inputs/S2_Tiling_GE_test.shp'\n",
    "field = 'Name'\n",
    "directory_images_s2 = os.path.join(cwd, 'S2_img')\n",
    "directory_images_s1_not_compressed = '/media/wenger/LaCie/S1_img'\n",
    "directory_images_s1_compressed = '/media/wenger/DATA2/S1_img'\n",
    "shapefile_roads = './inputs/routes_finales_GE.shp'\n",
    "patch_size = 256\n",
    "directory_ground_reference_raster = os.path.join(cwd, 'ground_reference_raster')\n",
    "directory_tmp = os.path.join(cwd, 'tmp')\n",
    "directory_dataset = os.path.join(cwd, 'dataset')\n",
    "path_shp_ground_reference = './inputs/merge_grand_est.shp'\n",
    "attribute_shp_ground_reference_urban = 'cod_n5'\n",
    "attribute_shp_ground_reference_natural = 'cod_n4'\n",
    "\n",
    "# Read all S2-Tiles and put them in an array\n",
    "# tiles_list = get_sentinel_tiles(path_shapefile_sentinel, field)\n",
    "#tiles_list = ['31UGP', '32ULU', '32ULV', '32UMU', '32UMV', '31UGQ', '31UFQ', '31UFP', '32TLT']\n",
    "tiles_list = ['32ULU']\n",
    "\n",
    "# Download Sentinel for each tile\n",
    "start_date = '2020-10-01'\n",
    "end_date = '2020-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-08T11:24:44.868154 Downloading S2 images from 2020-10-01 to 2020-12-31 for each S2 tiles.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2021-11-08T13:28:58.523219 Successfully downloaded S2 images from 2020-10-01 to 2020-12-31.\n",
      "2021-11-08T13:28:58.523401 Unzipping S2 images downloaded.\n",
      "2021-11-08T13:29:21.804038Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2B_20201118-104739-093_L2A_T31UFP_D.zip\n",
      "2021-11-08T13:29:40.338556Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2B_20201118-104732-754_L2A_T32ULU_D.zip\n",
      "2021-11-08T13:30:03.249535Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2B_20201118-104724-709_L2A_T31UFQ_D.zip\n",
      "2021-11-08T13:30:27.860423Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2B_20201105-103719-544_L2A_T32UMV_D.zip\n",
      "2021-11-08T13:30:50.402832Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2B_20201118-104735-372_L2A_T31UGP_D.zip\n",
      "2021-11-08T13:31:14.549606Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2B_20201105-103723-559_L2A_T32ULV_D.zip\n",
      "2021-11-08T13:31:35.951272Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2B_20201105-103740-268_L2A_T31UGP_D.zip\n",
      "2021-11-08T13:31:46.343937Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2B_20201118-104711-282_L2A_T32UMV_D.zip\n",
      "2021-11-08T13:32:02.745848Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2B_20201118-104747-014_L2A_T32TLT_D.zip\n",
      "2021-11-08T13:32:23.303860Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2B_20201105-103725-585_L2A_T31UGQ_D.zip\n",
      "2021-11-08T13:32:45.946475Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2A_20201031-103726-465_L2A_T32ULV_D.zip\n",
      "2021-11-08T13:32:56.170800Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2B_20201105-103746-120_L2A_T31UFP_D.zip\n",
      "2021-11-08T13:33:11.537094Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2A_20201107-102724-217_L2A_T32UMV_D.zip\n",
      "2021-11-08T13:33:32.153015Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2A_20201031-103728-490_L2A_T31UGQ_D.zip\n",
      "2021-11-08T13:33:51.389338Successfully extracted /home/wenger/Documents/doctorat_romain_wenger/create_dataset/SENTINEL2A_20201106-105724-851_L2A_T31UFQ_D.zip\n",
      "2021-11-08T13:33:51.389758 Successfully unzipped S2 images.\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().isoformat() + ' Downloading S2 images from ' + str(\n",
    "        start_date) + ' to ' + str(end_date) + ' for each S2 tiles.')\n",
    "for tile in tiles_list:\n",
    "    print('')\n",
    "    download_sentinel2(tile, start_date, end_date)\n",
    "print(datetime.now().isoformat() + ' Successfully downloaded S2 images from ' + str(\n",
    "    start_date) + ' to ' + str(end_date) + '.')\n",
    "\n",
    "print(datetime.now().isoformat() + ' Unzipping S2 images downloaded.')\n",
    "for file in os.listdir(cwd):\n",
    "    file_path = os.path.join(cwd, file)\n",
    "    if zipfile.is_zipfile(file_path):\n",
    "        unzip(file_path, directory_images_s2)\n",
    "print(datetime.now().isoformat() + ' Successfully unzipped S2 images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12T13:40:03.843206 Rasterizing ground reference for each S2 tiles.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'rmdir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-4f29124aabb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' Successfully rasterized ground reference for each S2 tiles.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'rmdir'"
     ]
    }
   ],
   "source": [
    "print(datetime.now().isoformat() + ' Rasterizing ground reference for each S2 tiles.')\n",
    "if not os.path.exists(directory_ground_reference_raster):\n",
    "    os.mkdir(directory_ground_reference_raster)\n",
    "\n",
    "if not os.path.exists(directory_tmp):\n",
    "    os.mkdir(directory_tmp)\n",
    "\n",
    "dic_classes_urban = extract_dic(urban_regroup_classes.split(' '))\n",
    "dic_classes_natural = extract_dic(natural_regroup_classes.split(' '))\n",
    "\n",
    "attributes_merge = []\n",
    "for key in dic_classes_urban:\n",
    "    if dic_classes_urban[key] not in attributes_merge:\n",
    "        attributes_merge.append(dic_classes_urban[key])\n",
    "\n",
    "if -99 in attributes_merge:\n",
    "    attributes_merge.remove(-99)\n",
    "\n",
    "all_classes_urban = []\n",
    "for key in dic_classes_urban:\n",
    "    if key not in all_classes_urban:\n",
    "        all_classes_urban.append(key)\n",
    "\n",
    "all_classes_natural = []\n",
    "for key in dic_classes_natural:\n",
    "    if key not in all_classes_natural:\n",
    "        all_classes_natural.append(key)\n",
    "\n",
    "for tile in tiles_list:\n",
    "    for img in os.listdir(directory_images_s2):\n",
    "        if tile in img:\n",
    "            path_band_ref = os.path.join(directory_images_s2, img, img + '_SRE_B3.tif')\n",
    "            output_filename_tmp = os.path.join(cwd, directory_tmp,\n",
    "                                                 tile + '_tmp.tif')\n",
    "            shapefile_to_raster_tmp = os.path.join(cwd, directory_tmp,\n",
    "                                                     path_shp_ground_reference.split('/')[2][:-4] + '_tmp.shp')\n",
    "            output_filename_merge_tmp = os.path.join(cwd, directory_tmp,\n",
    "                                                 tile + '_merge_tmp.tif')\n",
    "            output_filename_merge_final = os.path.join(cwd, directory_ground_reference_raster,\n",
    "                                                 tile + '_ground_reference.tif')\n",
    "\n",
    "            # Changing ground reference projection and reclassify\n",
    "            change_projection(path_band_ref, path_shp_ground_reference, attribute_shp_ground_reference_urban,\n",
    "                              attribute_shp_ground_reference_natural, shapefile_to_raster_tmp, all_classes_urban, all_classes_natural)\n",
    "\n",
    "            # Rasterizing shapefile\n",
    "            rasterize_ground_ref(path_band_ref, shapefile_to_raster_tmp, attribute_shp_ground_reference_urban,\n",
    "                                 output_filename_tmp)\n",
    "\n",
    "            merge_classes(output_filename_tmp, merge_two_dicts(dic_classes_urban, dic_classes_natural), output_filename_merge_tmp)\n",
    "\n",
    "            #Adding roads on reference data\n",
    "            pixel_roads_value = 25\n",
    "            computing_roads(os.path.join(cwd, directory_tmp), shapefile_roads, pixel_roads_value,\n",
    "                            output_filename_merge_tmp, output_filename_merge_final)\n",
    "\n",
    "            break\n",
    "\n",
    "if os.path.exists(directory_tmp):\n",
    "    shutil.rmtree(directory_tmp)\n",
    "\n",
    "print(datetime.now().isoformat() + ' Successfully rasterized ground reference for each S2 tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1 (Doesn't work yet, use s1tiling : https://s1-tiling.pages.orfeo-toolbox.org/s1tiling/latest/install.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-15T14:47:32.133849 Compress S1\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1a_31TFN_vh_ASC_161_20200118t173159.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1a_31TFN_vh_ASC_161_20200118t173159.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1a_31TFN_vh_ASC_161_20200130t173158.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1a_31TFN_vh_ASC_161_20200130t173158.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1a_31TFN_vh_DES_037_20200110txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1a_31TFN_vh_DES_037_20200110txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1a_31TFN_vh_DES_037_20200122txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1a_31TFN_vh_DES_037_20200122txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1a_31TFN_vv_ASC_161_20200118t173159.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1a_31TFN_vv_ASC_161_20200118t173159.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1a_31TFN_vv_ASC_161_20200130t173158.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1a_31TFN_vv_ASC_161_20200130t173158.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1a_31TFN_vv_DES_037_20200110txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1a_31TFN_vv_DES_037_20200110txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1a_31TFN_vv_DES_037_20200122txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1a_31TFN_vv_DES_037_20200122txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1b_31TFN_vh_ASC_161_20200112txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1b_31TFN_vh_ASC_161_20200112txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1b_31TFN_vh_ASC_161_20200124txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1b_31TFN_vh_ASC_161_20200124txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1b_31TFN_vh_DES_037_20200104txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1b_31TFN_vh_DES_037_20200104txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1b_31TFN_vh_DES_037_20200116txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1b_31TFN_vh_DES_037_20200116txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1b_31TFN_vh_DES_037_20200128txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1b_31TFN_vh_DES_037_20200128txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1b_31TFN_vv_ASC_161_20200112txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1b_31TFN_vv_ASC_161_20200112txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1b_31TFN_vv_ASC_161_20200124txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1b_31TFN_vv_ASC_161_20200124txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1b_31TFN_vv_DES_037_20200104txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1b_31TFN_vv_DES_037_20200104txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1b_31TFN_vv_DES_037_20200116txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1b_31TFN_vv_DES_037_20200116txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/01/s1b_31TFN_vv_DES_037_20200128txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/01/s1b_31TFN_vv_DES_037_20200128txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1a_31TFN_vh_ASC_161_20200211t173158.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1a_31TFN_vh_ASC_161_20200211t173158.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1a_31TFN_vh_ASC_161_20200223t173158.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1a_31TFN_vh_ASC_161_20200223t173158.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1a_31TFN_vh_DES_037_20200203txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1a_31TFN_vh_DES_037_20200203txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1a_31TFN_vh_DES_037_20200215txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1a_31TFN_vh_DES_037_20200215txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1a_31TFN_vh_DES_037_20200227txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1a_31TFN_vh_DES_037_20200227txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1a_31TFN_vv_ASC_161_20200211t173158.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1a_31TFN_vv_ASC_161_20200211t173158.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1a_31TFN_vv_ASC_161_20200223t173158.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1a_31TFN_vv_ASC_161_20200223t173158.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1a_31TFN_vv_DES_037_20200203txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1a_31TFN_vv_DES_037_20200203txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1a_31TFN_vv_DES_037_20200215txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1a_31TFN_vv_DES_037_20200215txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1a_31TFN_vv_DES_037_20200227txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1a_31TFN_vv_DES_037_20200227txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1b_31TFN_vh_ASC_161_20200205txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1b_31TFN_vh_ASC_161_20200205txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1b_31TFN_vh_ASC_161_20200217txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1b_31TFN_vh_ASC_161_20200217txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1b_31TFN_vh_ASC_161_20200229txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1b_31TFN_vh_ASC_161_20200229txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1b_31TFN_vh_DES_037_20200209txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1b_31TFN_vh_DES_037_20200209txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1b_31TFN_vh_DES_037_20200221txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1b_31TFN_vh_DES_037_20200221txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1b_31TFN_vv_ASC_161_20200205txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1b_31TFN_vv_ASC_161_20200205txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1b_31TFN_vv_ASC_161_20200217txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1b_31TFN_vv_ASC_161_20200217txxxxxx.tif\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1b_31TFN_vv_ASC_161_20200229txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1b_31TFN_vv_ASC_161_20200229txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1b_31TFN_vv_DES_037_20200209txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1b_31TFN_vv_DES_037_20200209txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/02/s1b_31TFN_vv_DES_037_20200221txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/02/s1b_31TFN_vv_DES_037_20200221txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1a_31TFN_vh_ASC_161_20200306t173158.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1a_31TFN_vh_ASC_161_20200306t173158.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1a_31TFN_vh_ASC_161_20200318t173158.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1a_31TFN_vh_ASC_161_20200318t173158.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1a_31TFN_vh_ASC_161_20200330t173158.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1a_31TFN_vh_ASC_161_20200330t173158.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1a_31TFN_vh_DES_037_20200310txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1a_31TFN_vh_DES_037_20200310txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1a_31TFN_vh_DES_037_20200322txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1a_31TFN_vh_DES_037_20200322txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1a_31TFN_vv_ASC_161_20200306t173158.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1a_31TFN_vv_ASC_161_20200306t173158.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1a_31TFN_vv_ASC_161_20200318t173158.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1a_31TFN_vv_ASC_161_20200318t173158.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1a_31TFN_vv_ASC_161_20200330t173158.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1a_31TFN_vv_ASC_161_20200330t173158.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1a_31TFN_vv_DES_037_20200310txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1a_31TFN_vv_DES_037_20200310txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1a_31TFN_vv_DES_037_20200322txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1a_31TFN_vv_DES_037_20200322txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1b_31TFN_vh_ASC_161_20200312txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1b_31TFN_vh_ASC_161_20200312txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1b_31TFN_vh_ASC_161_20200324txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1b_31TFN_vh_ASC_161_20200324txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1b_31TFN_vh_DES_037_20200304txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1b_31TFN_vh_DES_037_20200304txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1b_31TFN_vh_DES_037_20200316txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1b_31TFN_vh_DES_037_20200316txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1b_31TFN_vh_DES_037_20200328txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1b_31TFN_vh_DES_037_20200328txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1b_31TFN_vv_ASC_161_20200312txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1b_31TFN_vv_ASC_161_20200312txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1b_31TFN_vv_ASC_161_20200324txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1b_31TFN_vv_ASC_161_20200324txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1b_31TFN_vv_DES_037_20200304txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1b_31TFN_vv_DES_037_20200304txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1b_31TFN_vv_DES_037_20200316txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1b_31TFN_vv_DES_037_20200316txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/03/s1b_31TFN_vv_DES_037_20200328txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/03/s1b_31TFN_vv_DES_037_20200328txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1a_31TFN_vh_ASC_161_20200411t173159.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1a_31TFN_vh_ASC_161_20200411t173159.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1a_31TFN_vh_ASC_161_20200423t173159.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1a_31TFN_vh_ASC_161_20200423t173159.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1a_31TFN_vh_DES_037_20200403txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1a_31TFN_vh_DES_037_20200403txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1a_31TFN_vh_DES_037_20200415txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1a_31TFN_vh_DES_037_20200415txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1a_31TFN_vh_DES_037_20200427txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1a_31TFN_vh_DES_037_20200427txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1a_31TFN_vv_ASC_161_20200411t173159.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1a_31TFN_vv_ASC_161_20200411t173159.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1a_31TFN_vv_ASC_161_20200423t173159.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1a_31TFN_vv_ASC_161_20200423t173159.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1a_31TFN_vv_DES_037_20200403txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1a_31TFN_vv_DES_037_20200403txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1a_31TFN_vv_DES_037_20200415txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1a_31TFN_vv_DES_037_20200415txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1a_31TFN_vv_DES_037_20200427txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1a_31TFN_vv_DES_037_20200427txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1b_31TFN_vh_ASC_161_20200405txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1b_31TFN_vh_ASC_161_20200405txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1b_31TFN_vh_ASC_161_20200417txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1b_31TFN_vh_ASC_161_20200417txxxxxx.tif\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1b_31TFN_vh_ASC_161_20200429txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1b_31TFN_vh_ASC_161_20200429txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1b_31TFN_vh_DES_037_20200409txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1b_31TFN_vh_DES_037_20200409txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1b_31TFN_vh_DES_037_20200421txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1b_31TFN_vh_DES_037_20200421txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1b_31TFN_vv_ASC_161_20200405txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1b_31TFN_vv_ASC_161_20200405txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1b_31TFN_vv_ASC_161_20200417txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1b_31TFN_vv_ASC_161_20200417txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1b_31TFN_vv_ASC_161_20200429txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1b_31TFN_vv_ASC_161_20200429txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1b_31TFN_vv_DES_037_20200409txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1b_31TFN_vv_DES_037_20200409txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/04/s1b_31TFN_vv_DES_037_20200421txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/04/s1b_31TFN_vv_DES_037_20200421txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/05/s1a_31TFN_vh_ASC_161_20200505t173200.tif /media/wenger/DATA2/S1_img/31TFN/2020/05/s1a_31TFN_vh_ASC_161_20200505t173200.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/05/s1a_31TFN_vh_ASC_161_20200517t173200.tif /media/wenger/DATA2/S1_img/31TFN/2020/05/s1a_31TFN_vh_ASC_161_20200517t173200.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/05/s1a_31TFN_vh_ASC_161_20200529t173201.tif /media/wenger/DATA2/S1_img/31TFN/2020/05/s1a_31TFN_vh_ASC_161_20200529t173201.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/05/s1a_31TFN_vh_DES_037_20200509txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/05/s1a_31TFN_vh_DES_037_20200509txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/05/s1a_31TFN_vh_DES_037_20200521txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/05/s1a_31TFN_vh_DES_037_20200521txxxxxx.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/05/s1a_31TFN_vv_ASC_161_20200505t173200.tif /media/wenger/DATA2/S1_img/31TFN/2020/05/s1a_31TFN_vv_ASC_161_20200505t173200.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/05/s1a_31TFN_vv_ASC_161_20200517t173200.tif /media/wenger/DATA2/S1_img/31TFN/2020/05/s1a_31TFN_vv_ASC_161_20200517t173200.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/05/s1a_31TFN_vv_ASC_161_20200529t173201.tif /media/wenger/DATA2/S1_img/31TFN/2020/05/s1a_31TFN_vv_ASC_161_20200529t173201.tif\n",
      "gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" /media/wenger/LaCie/S1_img/31TFN/2020/05/s1a_31TFN_vv_DES_037_20200509txxxxxx.tif /media/wenger/DATA2/S1_img/31TFN/2020/05/s1a_31TFN_vv_DES_037_20200509txxxxxx.tif\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().isoformat() + ' Compress S1')\n",
    "create_folders = False\n",
    "\n",
    "if create_folders:\n",
    "    for tile in os.listdir(directory_images_s1_not_compressed):\n",
    "        os.mkdir(os.path.join(directory_images_s1_compressed, tile))\n",
    "        for year in os.listdir(os.path.join(directory_images_s1_not_compressed, tile)):\n",
    "            os.mkdir(os.path.join(directory_images_s1_compressed, tile, year))\n",
    "            for month in os.listdir(os.path.join(directory_images_s1_not_compressed, tile, year)):\n",
    "                os.mkdir(os.path.join(directory_images_s1_compressed, tile, year, month))\n",
    "            \n",
    "\n",
    "for tile in os.listdir(directory_images_s1_not_compressed):\n",
    "    for year in os.listdir(os.path.join(directory_images_s1_not_compressed, tile)):\n",
    "        for month in os.listdir(os.path.join(directory_images_s1_not_compressed, tile, year)):\n",
    "            for img in os.listdir(os.path.join(directory_images_s1_not_compressed, tile, year, month)):\n",
    "                command = 'gdal_translate -co \"COMPRESS=ZSTD\" -co \"PREDICTOR=3\" -co \"ZSTD_LEVEL=8\" ' + os.path.join(directory_images_s1_not_compressed, tile, year, month, img) + ' ' +  os.path.join(directory_images_s1_compressed, tile, year, month, img)\n",
    "                print(command)\n",
    "                os.system(command)\n",
    "\n",
    "print(datetime.now().isoformat() + ' Successfully compressed S1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-09T09:23:16.579565 Calculating patches for each tile.\n",
      "2021-11-09T09:25:00.853008 Successfully calculated patches for each tile.\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().isoformat() + ' Calculating patches for each tile.')\n",
    "'''if not os.path.exists(directory_dataset):\n",
    "    os.mkdir(directory_dataset)\n",
    "\n",
    "create_folders_dataset(directory_dataset)\n",
    "\n",
    "for tile in tiles_list:\n",
    "    print(datetime.now().isoformat() + ' Current tile is ' + str(tile))\n",
    "    calculate_patches_tile(tile, directory_ground_reference_raster, directory_images_s2,\n",
    "                           directory_images_s1,\n",
    "                           directory_dataset, patch_size, method=1)'''\n",
    "\n",
    "create_json_labels(directory_dataset)\n",
    "print(datetime.now().isoformat() + ' Successfully calculated patches for each tile.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-09T16:49:43.038541 Checking overlapped patches.\n",
      "2021-11-09T21:59:42.737621 Successfully checked and deleted overlapped patches\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().isoformat() + ' Checking overlapped patches.')\n",
    "delete_zero_patch(directory_dataset)\n",
    "delete_overlapping_patches(directory_dataset)\n",
    "print(datetime.now().isoformat() + ' Successfully checked and deleted overlapped patches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate stats\n",
    "\n",
    "This part is dedicated to calculate some statistics over the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_per_month(path_sat_img, sat):\n",
    "    \"\"\"\n",
    "    Extract the number of image for the chosen satellite\n",
    "    @params:\n",
    "        path_dataset   \t- Required  : path to the dataset\n",
    "        sat             - Required  : satellite to create stats, 's1' or 's2'\n",
    "    \"\"\"\n",
    "    assert (sat == 's1') or (sat == 's2')\n",
    "\n",
    "    dic_res = {}\n",
    "\n",
    "    if sat == 's1':\n",
    "        for tile in os.listdir(path_sat_img):\n",
    "            for year in os.listdir(os.path.join(path_sat_img, tile)):\n",
    "                for month in os.listdir(os.path.join(path_sat_img, tile, year)):\n",
    "                    path_img = os.path.join(path_sat_img, tile, year, month)\n",
    "                    nb_file = len([name for name in os.listdir(path_img) if os.path.isfile(os.path.join(path_img, name))])\n",
    "                    nb_file = nb_file/2\n",
    "\n",
    "                    if month in dic_res.keys():\n",
    "                        dic_res[int(month)] += nb_file\n",
    "                    else:\n",
    "                        dic_res[int(month)] = nb_file\n",
    "    else:\n",
    "        for folder_img in os.listdir(os.path.join(path_sat_img)):\n",
    "            if 'SENTINEL' in folder_img:\n",
    "                month = int(get_date_from_image_path(folder_img, s1=False)[4:6])\n",
    "\n",
    "                if month in dic_res.keys():\n",
    "                    dic_res[month] += 1\n",
    "                else:\n",
    "                    dic_res[month] = 1\n",
    "\n",
    "    return dic_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_pixel_gr(path_dataset):\n",
    "    \"\"\"\n",
    "    Count the number of pixel per class in the dataset.\n",
    "    @params:\n",
    "        path_dataset   \t- Required  : path to the dataset\n",
    "    \"\"\"\n",
    "    dic_res = {}\n",
    "\n",
    "    for patch in os.listdir(os.path.join(path_dataset, 'ground_reference')):\n",
    "        dataset = gdal.Open(os.path.join(path_dataset, 'ground_reference', patch), GA_ReadOnly)\n",
    "        array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "        unique, counts = np.unique(array, return_counts=True)\n",
    "        dic_patch = dict(zip(unique, counts))\n",
    "\n",
    "        if 0 not in array:\n",
    "            for key in dic_patch.keys():\n",
    "                if key in dic_res.keys():\n",
    "                    dic_res[key] += dic_patch[key]\n",
    "                else:\n",
    "                    dic_res[key] = dic_patch[key]\n",
    "\n",
    "    return dic_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bar_plot(dic, path_output, xlabel, ylabel, title, log=True):\n",
    "    \"\"\"\n",
    "    Make bar plot with data contained in a dictionnary.\n",
    "    @params:\n",
    "        dic   \t        - Required  : dictionnary with data\n",
    "        path_output   \t- Required  : output path of the png file\n",
    "    \"\"\"\n",
    "    height = dic.values()\n",
    "    bars = dic.keys()\n",
    "    y_pos = np.arange(len(bars))\n",
    "    \n",
    "    plt.style.use('ggplot')\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = 'Helvetica'\n",
    "\n",
    "    # Create bars\n",
    "    plt.bar(y_pos, height, width= 0.7, align='center', alpha=0.4, color='blue', edgecolor = 'red')\n",
    "    \n",
    "    if log:\n",
    "        plt.yscale(\"log\") \n",
    "        # This is the location for the annotated text\n",
    "        i = 1.0\n",
    "        # Annotating the bar plot with the values (total death count)\n",
    "        for i in range(len(bars)):\n",
    "            y_stride = height[i] * 0.1\n",
    "            plt.annotate(\"{:.2e}\".format(height[i]), (-0.5 + i, height[i] + y_stride), size=4)\n",
    "    else:\n",
    "        i = 1.0\n",
    "        # Annotating the bar plot with the values (total death count)\n",
    "        for i in range(len(bars)):\n",
    "            y_stride = height[i] * 0.02\n",
    "            plt.annotate(height[i], (-0.1 + i, height[i] + y_stride), size=5)\n",
    "    \n",
    "    # Create names on the x-axis\n",
    "    plt.xticks(y_pos, bars)\n",
    "    \n",
    "    plt.xlabel(str(xlabel), fontsize=9)\n",
    "    plt.ylabel(str(ylabel), fontsize=9)\n",
    "    plt.title(str(title), fontsize=9)\n",
    "\n",
    "    plt.draw()\n",
    "    plt.savefig(path_output, dpi=300)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_stats = './stats_dataset'\n",
    "\n",
    "if not os.path.exists(output_stats):\n",
    "    os.mkdir(output_stats)\n",
    "else:\n",
    "    shutil.rmtree(output_stats)\n",
    "    os.mkdir(output_stats)\n",
    "\n",
    "path_dataset = './dataset'\n",
    "path_s1 = '/media/wenger/LaCie/S1_img'\n",
    "path_s2 = './S2_img'\n",
    "\n",
    "dic_months_s1 = get_images_per_month(path_s1, sat='s1')\n",
    "dic_months_s2 = get_images_per_month(path_s2, sat='s2')\n",
    "dic_pixels = get_number_pixel_gr(path_dataset)\n",
    "\n",
    "make_bar_plot(dic_months_s1, os.path.join(output_stats, 's1_bar_plot.png'), 'Months', 'Number of images', 'Number of images per month for Sentinel-1 time serie', log=False)\n",
    "make_bar_plot(dic_months_s2, os.path.join(output_stats, 's2_bar_plot.png'), 'Months', 'Number of images', 'Number of images per month for Sentinel-2 time serie', log=False)\n",
    "make_bar_plot(dic_pixels, os.path.join(output_stats, 'classes_bar_plot.png'), 'Classes', 'Number of pixels', 'Number of pixels per classes for ground reference')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
